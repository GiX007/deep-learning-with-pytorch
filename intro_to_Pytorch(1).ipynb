{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8wEglSEPfcyv",
        "_3n0Pi46ojnt",
        "1NSJFxhK4Isu",
        "NRw3XuPYZVnW",
        "dB-EA_ihQ17p",
        "FEDzRskPVtWi"
      ],
      "authorship_tag": "ABX9TyOktsyQUY83FHICPvs/oZWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiX7000/deep-learning-with-pytorch/blob/main/intro_to_Pytorch(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A quick introduction to Pytorch, its features, key concepts and associated tools and libraries, Part 1."
      ],
      "metadata": {
        "id": "lfpZ0ZTgVyoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch is an open source machine learning framework that accelarates the path from research prototyping to production deployment. Find more here: https://pytorch.org/."
      ],
      "metadata": {
        "id": "A56rWfT1Yjwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tensor Basics."
      ],
      "metadata": {
        "id": "8wEglSEPfcyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJrCoGezyjrd"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create tensors. everything in deep learning has to do with twnsors\n",
        "x1 = torch.empty(3, 3)\n",
        "x2 = torch.randn(3,2)\n",
        "x3 = torch.ones(3,2)\n",
        "x4 = torch.zeros(3,2)\n",
        "x5 = torch.tensor([3, 5])\n",
        "print(x1)\n",
        "print(x2)\n",
        "print(x3.dtype)\n",
        "print(x4.size())\n",
        "print(x5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXIA7XPIc_JE",
        "outputId": "6fd22d65-9ada-4ba0-85b1-475477cf37bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.1722e-34, 0.0000e+00, 1.0725e+00],\n",
            "        [4.1289e-01, 1.2386e+00, 2.9072e-01],\n",
            "        [3.3009e-34, 0.0000e+00, 3.3013e-34]])\n",
            "tensor([[ 0.3809, -0.3081],\n",
            "        [ 1.9220, -0.4831],\n",
            "        [-0.3849, -0.8822]])\n",
            "torch.float32\n",
            "torch.Size([3, 2])\n",
            "tensor([3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x2[:, 1]) # all elements from the second row\n",
        "print(x2[0, :]) # all elements from the first row\n",
        "print(x2[0,0].item()) # actual value of an element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09-lrLDCywzA",
        "outputId": "67dbad92-469c-4ea1-fdbe-fb0884a98ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.3081, -0.4831, -0.8822])\n",
            "tensor([ 0.3809, -0.3081])\n",
            "0.3809219002723694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some operations between tensors\n",
        "y1 = torch.add(x2, x3)\n",
        "y2 = torch.sub(x2, x3)\n",
        "y3 = torch.mul(x2, x3)  # try also div operation\n",
        "print(y1)\n",
        "print(y2)\n",
        "print(y3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZM7_WFbemxf",
        "outputId": "3bf42938-c902-4cdd-9aca-784e669c8a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.7063,  1.5566],\n",
            "        [-0.0725,  0.5871],\n",
            "        [-0.2386,  0.7093]])\n",
            "tensor([[-0.2937, -0.4434],\n",
            "        [-2.0725, -1.4129],\n",
            "        [-2.2386, -1.2907]])\n",
            "tensor([[ 0.7063,  0.5566],\n",
            "        [-1.0725, -0.4129],\n",
            "        [-1.2386, -0.2907]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add a single number in a tensor with add_\n",
        "a = y3.add_(10)\n",
        "print(a) \n",
        "print(a.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHYzkk6gkliQ",
        "outputId": "2174b4c0-a90c-4159-ee6b-c2fc8c9dc7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[20.7063, 20.5566],\n",
            "        [18.9275, 19.5871],\n",
            "        [18.7614, 19.7093]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4,4)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXSBAx46iDAw",
        "outputId": "69afaf3e-6d56-4d43-ed24-899397371546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.1057,  0.6431,  2.0922,  0.7599],\n",
            "        [-0.0301, -0.1411,  0.5497,  1.5615],\n",
            "        [-1.1288,  1.2645,  0.7969,  0.6944],\n",
            "        [ 0.7365,  0.0767,  0.7164,  1.3825]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x.view(-1, 8) # (-1, 8) is equal to (16). here we should have the same dim with the above x(4,4)\n",
        "print(y)\n",
        "print(y.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IAOevreiulf",
        "outputId": "aa63d112-6ee3-4a62-96c0-df2573bd5f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.1057,  0.6431,  2.0922,  0.7599, -0.0301, -0.1411,  0.5497,  1.5615],\n",
            "        [-1.1288,  1.2645,  0.7969,  0.6944,  0.7365,  0.0767,  0.7164,  1.3825]])\n",
            "torch.Size([2, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert a tensor to numpy\n",
        "b = x3.numpy()\n",
        "print(x3)\n",
        "print(b)\n",
        "print(type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy1O8yadjXSX",
        "outputId": "ff470ec5-4872-43d2-a636-1ee0b1c7cfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert a numpy array to tensor\n",
        "import numpy as np\n",
        "\n",
        "c = np.ones(5)\n",
        "print(c)\n",
        "d = torch.from_numpy(c)\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2N4a8XdfaMV",
        "outputId": "e885fb40-e18d-4300-85d4-75978c1c7a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we add 1 in numpy array, look that 1 is also added in its tensor\n",
        "c += 1\n",
        "print(c)\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7i3xSFVluJE",
        "outputId": "cfcbd940-05e3-4d1f-9c0a-191f2aff1f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = torch.ones(5, requires_grad=True) # we tell it that we need to calculate gradients later\n",
        "e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbPeHKdHluE6",
        "outputId": "e1f55d52-cc4e-4987-ba86-d6dc2d19f48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1., 1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fundamentals of Autograd."
      ],
      "metadata": {
        "id": "_3n0Pi46ojnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "cfoZhpCEwtJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)  # set it True because we need the gradients for backpropagation. If you do not define it here, you'll get an error later in backward() \n",
        "print(x)\n",
        "\n",
        "y = x+2 # creates a node with x, 2 inputs and an operation of +\n",
        "print(y)\n",
        "\n",
        "z = y*y*2\n",
        "print(z)\n",
        "\n",
        "z2 = z.mean()\n",
        "print(z2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcu3zsBjluBS",
        "outputId": "0795ef50-016d-492b-dc5b-55212c869c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.7568, -1.1296, -0.9280], requires_grad=True)\n",
            "tensor([2.7568, 0.8704, 1.0720], grad_fn=<AddBackward0>)\n",
            "tensor([15.1999,  1.5152,  2.2983], grad_fn=<MulBackward0>)\n",
            "tensor(6.3378, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the gradients\n",
        "z2.backward()  # dz2/dx\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJEQ3lkglt-C",
        "outputId": "f6d141f4-2597-4b2b-84b4-a37daf21bee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9145, 2.8044, 3.4890])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward() only for scalars. try this:\n",
        "z.backward()\n",
        "print(x.grads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "r23bOr9iql_7",
        "outputId": "5052dbd6-b018-4281-d130-a513b61127bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a7bb712c857d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# backward() only for scalars. try this:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# but if the last calculation is not a scalar, we need to give to backward() an argument, which must have the same dims as x(which has requires_grad=True)\n",
        "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
        "z.backward(v)\n",
        "print(x.grad) # this is a vector jacobian product!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1-wONWXql9I",
        "outputId": "8529213e-587a-4293-9313-14199409899a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.1027, 3.4816, 0.0043])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 ways with which we can stop pytorch from creating gradient functions and tracking the history in computational graph.\n"
      ],
      "metadata": {
        "id": "RvuoRtEJz21C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = y+2\n",
        "print(y)\n",
        "\n",
        "# 1st way\n",
        "x.requires_grad_(False)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXN_fwzwql6X",
        "outputId": "42d50457-8d98-44ce-8f43-f1f006072758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.5793,  0.8202,  1.3019], requires_grad=True)\n",
            "tensor([6.7568, 4.8704, 5.0720], grad_fn=<AddBackward0>)\n",
            "tensor([-0.5793,  0.8202,  1.3019])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd way\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "z = x.detach()\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUoTQicgql04",
        "outputId": "8fdde602-d00f-4a7b-c5e1-380c0938352d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2998, -0.0143,  0.3201], requires_grad=True)\n",
            "tensor([-0.2998, -0.0143,  0.3201])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd way\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y = x+2\n",
        "  print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUM3e2YUql3T",
        "outputId": "fde9ebb3-d8f0-401e-baae-b3e797cfd927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8409, 0.3761, 2.1859], requires_grad=True)\n",
            "tensor([2.8409, 2.3761, 4.1859])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(5):  # try 1, 2, 3, 4\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad)    # print the grads of weights\n",
        "\n",
        "  # before go to the next iteration, we must empty the gradients\n",
        "  weights.grad.zero_()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVA46maoqlxw",
        "outputId": "8b54e6ac-5ac1-4a17-8c21-2096b4b1da60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 things to remember here: every time you want to calculate the gradients, set requires_grad=True, call backward() and empty the gradients before the next operation by using grad.zero_()."
      ],
      "metadata": {
        "id": "SBmLMHGT3eL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Implementations of the simplests ever neural network in Pytorch."
      ],
      "metadata": {
        "id": "1NSJFxhK4Isu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm with which the training of the majority of neural nets is based on is comprised of the following steps: 1) forward pass and compute the loss, 2) compute local gradients, 3) backward pass by computing dLoss/dweights using the chain rule, 4) update the weights and 5) do it until accumulate weights' best values."
      ],
      "metadata": {
        "id": "4_Hx5LQT56o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# manually execution of the training process of the simplest ever network\n",
        "import numpy as np\n",
        "\n",
        "# our data and their corresponding real values\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "\n",
        "# initialize the weight\n",
        "w = 0.0\n",
        "\n",
        "# forward pass=model's prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss = MSE => J =  1/N * (w*x - y)**2\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "# gradient => dJ/dw = 1/N * 2x(w*x-y)\n",
        "def gradient(x, y, y_predicted):\n",
        "  return np.dot(2*x, y_predicted-y).mean()\n",
        "\n",
        "print(f'prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction/forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # compute loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients computation with respect to w\n",
        "  dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 1 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {forward(5):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyJi5TYz4i-N",
        "outputId": "6fb3028e-8976-4cb8-b1c7-1103aa1728e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 30.00000000\n",
            "epoch 2: w = 1.680, loss = 4.79999924\n",
            "epoch 3: w = 1.872, loss = 0.76800019\n",
            "epoch 4: w = 1.949, loss = 0.12288000\n",
            "epoch 5: w = 1.980, loss = 0.01966083\n",
            "epoch 6: w = 1.992, loss = 0.00314574\n",
            "epoch 7: w = 1.997, loss = 0.00050331\n",
            "epoch 8: w = 1.999, loss = 0.00008053\n",
            "epoch 9: w = 1.999, loss = 0.00001288\n",
            "epoch 10: w = 2.000, loss = 0.00000206\n",
            "prediction after training: f(5) = 9.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the above's implementation in pytorch (1)\n",
        "import torch\n",
        "\n",
        "# our data and their corresponding real values\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "# initialize the weight\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True) # and of course, we don't forget to set requires_grad=True\n",
        "\n",
        "# forward pass=model's prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss = MSE => J =  1/N * (w*x - y)**2\n",
        "def loss(y, y_predicted):\n",
        "  return ((y_predicted-y)**2).mean()\n",
        "\n",
        "print(f'prediction before training: f(5) = {forward(5):.3f}') \n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 60  # if the result is not correct, increase it! we started from 10 for example\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction/forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # compute loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # the difference is that we do not manually calculate the gradient here\n",
        "  # gradients = backward pass\n",
        "  l.backward()  # dl/dw\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # zero gradients\n",
        "  w.grad.zero_()  # before the next iteration, make sure the gradients are 0 again!\n",
        "\n",
        "  if epoch % 1 ==0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {forward(5):.3f}')  # the expected output is 2 times the input value, so for input 5, we expect output 10!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivk3MAHu4i63",
        "outputId": "38273cf9-2d24-43e0-b7cd-84475ccd2f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 30.00000000\n",
            "epoch 2: w = 0.555, loss = 21.67499924\n",
            "epoch 3: w = 0.772, loss = 15.66018772\n",
            "epoch 4: w = 0.956, loss = 11.31448650\n",
            "epoch 5: w = 1.113, loss = 8.17471695\n",
            "epoch 6: w = 1.246, loss = 5.90623236\n",
            "epoch 7: w = 1.359, loss = 4.26725292\n",
            "epoch 8: w = 1.455, loss = 3.08308983\n",
            "epoch 9: w = 1.537, loss = 2.22753215\n",
            "epoch 10: w = 1.606, loss = 1.60939169\n",
            "epoch 11: w = 1.665, loss = 1.16278565\n",
            "epoch 12: w = 1.716, loss = 0.84011245\n",
            "epoch 13: w = 1.758, loss = 0.60698116\n",
            "epoch 14: w = 1.794, loss = 0.43854395\n",
            "epoch 15: w = 1.825, loss = 0.31684780\n",
            "epoch 16: w = 1.851, loss = 0.22892261\n",
            "epoch 17: w = 1.874, loss = 0.16539653\n",
            "epoch 18: w = 1.893, loss = 0.11949898\n",
            "epoch 19: w = 1.909, loss = 0.08633806\n",
            "epoch 20: w = 1.922, loss = 0.06237914\n",
            "epoch 21: w = 1.934, loss = 0.04506890\n",
            "epoch 22: w = 1.944, loss = 0.03256231\n",
            "epoch 23: w = 1.952, loss = 0.02352631\n",
            "epoch 24: w = 1.960, loss = 0.01699772\n",
            "epoch 25: w = 1.966, loss = 0.01228084\n",
            "epoch 26: w = 1.971, loss = 0.00887291\n",
            "epoch 27: w = 1.975, loss = 0.00641066\n",
            "epoch 28: w = 1.979, loss = 0.00463169\n",
            "epoch 29: w = 1.982, loss = 0.00334642\n",
            "epoch 30: w = 1.985, loss = 0.00241778\n",
            "epoch 31: w = 1.987, loss = 0.00174685\n",
            "epoch 32: w = 1.989, loss = 0.00126211\n",
            "epoch 33: w = 1.991, loss = 0.00091188\n",
            "epoch 34: w = 1.992, loss = 0.00065882\n",
            "epoch 35: w = 1.993, loss = 0.00047601\n",
            "epoch 36: w = 1.994, loss = 0.00034392\n",
            "epoch 37: w = 1.995, loss = 0.00024848\n",
            "epoch 38: w = 1.996, loss = 0.00017952\n",
            "epoch 39: w = 1.996, loss = 0.00012971\n",
            "epoch 40: w = 1.997, loss = 0.00009371\n",
            "epoch 41: w = 1.997, loss = 0.00006770\n",
            "epoch 42: w = 1.998, loss = 0.00004891\n",
            "epoch 43: w = 1.998, loss = 0.00003534\n",
            "epoch 44: w = 1.998, loss = 0.00002553\n",
            "epoch 45: w = 1.999, loss = 0.00001845\n",
            "epoch 46: w = 1.999, loss = 0.00001333\n",
            "epoch 47: w = 1.999, loss = 0.00000963\n",
            "epoch 48: w = 1.999, loss = 0.00000696\n",
            "epoch 49: w = 1.999, loss = 0.00000503\n",
            "epoch 50: w = 1.999, loss = 0.00000363\n",
            "epoch 51: w = 1.999, loss = 0.00000262\n",
            "epoch 52: w = 2.000, loss = 0.00000190\n",
            "epoch 53: w = 2.000, loss = 0.00000137\n",
            "epoch 54: w = 2.000, loss = 0.00000099\n",
            "epoch 55: w = 2.000, loss = 0.00000071\n",
            "epoch 56: w = 2.000, loss = 0.00000052\n",
            "epoch 57: w = 2.000, loss = 0.00000037\n",
            "epoch 58: w = 2.000, loss = 0.00000027\n",
            "epoch 59: w = 2.000, loss = 0.00000019\n",
            "epoch 60: w = 2.000, loss = 0.00000014\n",
            "prediction after training: f(5) = 9.999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the above's implementation in pytorch using a pytorch model (2)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# design th emodel\n",
        "\n",
        "# construct loss, optimizer\n",
        "\n",
        "# training loop; forward pass, backward pass, update weights\n",
        "\n",
        "# our data and their corresponding real values\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
        "\n",
        "# create a test sample\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "# instead of forward pass function and initialization of w, we use a pytorch model here that does all the work for us\n",
        "model = nn.Linear(input_size, output_size)  \n",
        "\n",
        "\n",
        "# the same above line can be replaced by the below my_model\n",
        "# this is how to design your custom pytorch model\n",
        "class LinearRegression(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim): \n",
        "    super(LinearRegression, self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "my_model = LinearRegression(input_size, output_size)  # check it by replacing below model with model2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'prediction before training: f(5) = {model(X_test).item():.3f}') \n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 70  # if the result is not correct, increase it!\n",
        "\n",
        "loss = nn.MSELoss() # replaces our custom loss function from the previous example. it does exactly the same\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction/forward pass. now, we juct call model(X)\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # compute loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # the difference is that we do not manually calculate the gradient here\n",
        "  # gradients = backward pass\n",
        "  l.backward()  # dl/dw\n",
        "\n",
        "  # update weights. this time optimizer does it for us\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()  # before the next iteration, make sure the gradients are 0 again!\n",
        "\n",
        "  if epoch % 1 ==0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'prediction after training: f(5) = {model(X_test).item():.3f}')  # the expected output is 2 times the input value, so for input 5, we expect output 10!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzHgKNgp4i3Z",
        "outputId": "83e918ff-d0ce-4674-f12b-d6692c732a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "prediction before training: f(5) = 2.829\n",
            "epoch 1: w = 0.905, loss = 17.13798714\n",
            "epoch 2: w = 1.094, loss = 11.89221096\n",
            "epoch 3: w = 1.251, loss = 8.25227547\n",
            "epoch 4: w = 1.381, loss = 5.72659826\n",
            "epoch 5: w = 1.490, loss = 3.97408223\n",
            "epoch 6: w = 1.581, loss = 2.75804472\n",
            "epoch 7: w = 1.657, loss = 1.91425753\n",
            "epoch 8: w = 1.720, loss = 1.32876968\n",
            "epoch 9: w = 1.772, loss = 0.92250854\n",
            "epoch 10: w = 1.816, loss = 0.64060968\n",
            "epoch 11: w = 1.852, loss = 0.44500309\n",
            "epoch 12: w = 1.882, loss = 0.30927289\n",
            "epoch 13: w = 1.907, loss = 0.21508938\n",
            "epoch 14: w = 1.928, loss = 0.14973462\n",
            "epoch 15: w = 1.946, loss = 0.10438359\n",
            "epoch 16: w = 1.960, loss = 0.07291250\n",
            "epoch 17: w = 1.972, loss = 0.05107242\n",
            "epoch 18: w = 1.982, loss = 0.03591515\n",
            "epoch 19: w = 1.991, loss = 0.02539507\n",
            "epoch 20: w = 1.998, loss = 0.01809257\n",
            "epoch 21: w = 2.003, loss = 0.01302266\n",
            "epoch 22: w = 2.008, loss = 0.00950197\n",
            "epoch 23: w = 2.012, loss = 0.00705621\n",
            "epoch 24: w = 2.016, loss = 0.00535647\n",
            "epoch 25: w = 2.018, loss = 0.00417425\n",
            "epoch 26: w = 2.020, loss = 0.00335121\n",
            "epoch 27: w = 2.022, loss = 0.00277740\n",
            "epoch 28: w = 2.024, loss = 0.00237654\n",
            "epoch 29: w = 2.025, loss = 0.00209572\n",
            "epoch 30: w = 2.026, loss = 0.00189818\n",
            "epoch 31: w = 2.027, loss = 0.00175847\n",
            "epoch 32: w = 2.028, loss = 0.00165888\n",
            "epoch 33: w = 2.028, loss = 0.00158715\n",
            "epoch 34: w = 2.029, loss = 0.00153478\n",
            "epoch 35: w = 2.029, loss = 0.00149586\n",
            "epoch 36: w = 2.029, loss = 0.00146626\n",
            "epoch 37: w = 2.030, loss = 0.00144316\n",
            "epoch 38: w = 2.030, loss = 0.00142459\n",
            "epoch 39: w = 2.030, loss = 0.00140919\n",
            "epoch 40: w = 2.030, loss = 0.00139597\n",
            "epoch 41: w = 2.030, loss = 0.00138430\n",
            "epoch 42: w = 2.030, loss = 0.00137372\n",
            "epoch 43: w = 2.030, loss = 0.00136390\n",
            "epoch 44: w = 2.030, loss = 0.00135464\n",
            "epoch 45: w = 2.030, loss = 0.00134578\n",
            "epoch 46: w = 2.030, loss = 0.00133720\n",
            "epoch 47: w = 2.030, loss = 0.00132883\n",
            "epoch 48: w = 2.030, loss = 0.00132063\n",
            "epoch 49: w = 2.030, loss = 0.00131255\n",
            "epoch 50: w = 2.030, loss = 0.00130458\n",
            "epoch 51: w = 2.030, loss = 0.00129670\n",
            "epoch 52: w = 2.030, loss = 0.00128888\n",
            "epoch 53: w = 2.030, loss = 0.00128113\n",
            "epoch 54: w = 2.030, loss = 0.00127345\n",
            "epoch 55: w = 2.029, loss = 0.00126581\n",
            "epoch 56: w = 2.029, loss = 0.00125823\n",
            "epoch 57: w = 2.029, loss = 0.00125070\n",
            "epoch 58: w = 2.029, loss = 0.00124321\n",
            "epoch 59: w = 2.029, loss = 0.00123578\n",
            "epoch 60: w = 2.029, loss = 0.00122839\n",
            "epoch 61: w = 2.029, loss = 0.00122105\n",
            "epoch 62: w = 2.029, loss = 0.00121373\n",
            "epoch 63: w = 2.029, loss = 0.00120648\n",
            "epoch 64: w = 2.029, loss = 0.00119926\n",
            "epoch 65: w = 2.029, loss = 0.00119210\n",
            "epoch 66: w = 2.029, loss = 0.00118497\n",
            "epoch 67: w = 2.028, loss = 0.00117788\n",
            "epoch 68: w = 2.028, loss = 0.00117084\n",
            "epoch 69: w = 2.028, loss = 0.00116384\n",
            "epoch 70: w = 2.028, loss = 0.00115687\n",
            "prediction after training: f(5) = 10.058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the prediction is not perfect, so you can play with the number of iterations and the learning rate."
      ],
      "metadata": {
        "id": "1h4FII4FVqTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. A very simple Linear Regression with Pytorch."
      ],
      "metadata": {
        "id": "NRw3XuPYZVnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "w0J90DeHb6jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 0) Create the data by using the make_regression method \n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=4)\n",
        "\n",
        "# cast to a specified float Tensor: from float64 to float32, because errors will occur later\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32)) \n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "# set y_numpy from a column vector(100,) to a matrix vector(100,1)\n",
        "y = y.view(y.shape[0], 1) # view method for reshaping tensors\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# 1) Model\n",
        "# Linear model f = wx + b\n",
        "input_size = n_features\n",
        "output_size = 1 # one output: a scalar prediction\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# 2) Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# 3) Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # 3.1 Forward pass and loss computation\n",
        "    y_predicted = model(X)\n",
        "    loss = criterion(y_predicted, y)\n",
        "    \n",
        "    # 3.2 Backward pass and update weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 3.3 zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0: # print every 10 iterations\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# Plot\n",
        "\n",
        "# convert back to numpy\n",
        "# prevent this operation from being tracked in our computational graph with detach()\n",
        "predicted = model(X).detach().numpy()\n",
        "\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k2fRe37g4i0E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "b2cd63da-a04e-4a73-af69-7020e1ec7fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 4088.8018\n",
            "epoch: 20, loss = 2880.5369\n",
            "epoch: 30, loss = 2056.9587\n",
            "epoch: 40, loss = 1495.4702\n",
            "epoch: 50, loss = 1112.5852\n",
            "epoch: 60, loss = 851.4383\n",
            "epoch: 70, loss = 673.2860\n",
            "epoch: 80, loss = 551.7286\n",
            "epoch: 90, loss = 468.7708\n",
            "epoch: 100, loss = 412.1450\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. A very simple Logistic Regression with Pytorch."
      ],
      "metadata": {
        "id": "dB-EA_ihQ17p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a binary classification problem which predicts cancer based on the input features."
      ],
      "metadata": {
        "id": "fEqmTMguTWpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 0) Load and Prepare the data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "# create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale all values with StandardScaler-> zero mean and unit variance\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# tranform our data into tensors and in float32 dtype\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# convert column target vectors from (n,) to matrix vectors of (n,1) \n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "# 1) define the Model\n",
        "# Linear model f = wx + b , sigmoid at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "# 2) Loss and optimizer\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()  # binary_crossentropy\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
        "\n",
        "# 3) Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass and loss\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    # Backward pass(calculate the gradients) and update the weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero grad before new step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
        "\n",
        "# evaluate the model. this process is not a part of our computation graph\n",
        "with torch.no_grad():\n",
        "    y_predicted = model(X_test)\n",
        "    # all predictions are either 0 or 1, that is why we round them\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    # accuracy = (number of y_predicted==y_test) / (number of y_test)\n",
        "    # equal function: eq()\n",
        "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    print(f'accuracy: {acc.item():.4f}')  # use of item() method as accuracy is just a scalar!"
      ],
      "metadata": {
        "id": "jaZONVX601Kc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bb75c6-9f35-42c3-f4c8-400ae4231c22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "569 30\n",
            "epoch: 10, loss = 0.5280\n",
            "epoch: 20, loss = 0.4513\n",
            "epoch: 30, loss = 0.3992\n",
            "epoch: 40, loss = 0.3611\n",
            "epoch: 50, loss = 0.3320\n",
            "epoch: 60, loss = 0.3087\n",
            "epoch: 70, loss = 0.2897\n",
            "epoch: 80, loss = 0.2738\n",
            "epoch: 90, loss = 0.2602\n",
            "epoch: 100, loss = 0.2484\n",
            "accuracy: 0.8860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Dataloaders, batch training and transforms."
      ],
      "metadata": {
        "id": "FEDzRskPVtWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have large datasets, it is more convenient to divide samples into smaller batches. So, for 100 samples and a batch size of 20, we have 5 iterations for 1 epoch."
      ],
      "metadata": {
        "id": "3neY39klY7VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# gradient computation etc. not efficient for whole data set\n",
        "# -> divide dataset into small batches\n",
        "\n",
        "'''\n",
        "# training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # loop over all batches\n",
        "    for i in range(total_batches):\n",
        "        batch_x, batch_y = ...\n",
        "'''\n",
        "\n",
        "# epoch = one forward and backward pass of ALL training samples\n",
        "# batch_size = number of training samples used in one forward/backward pass\n",
        "# number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of sampes\n",
        "# e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch\n",
        "\n",
        "# --> DataLoader can do the batch computation for us\n",
        "\n",
        "# Implement a custom Dataset:\n",
        "# inherit Dataset\n",
        "# implement __init__ , __getitem__ , and __len__\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize data, download, etc.\n",
        "        # read with numpy or pandas\n",
        "        xy = np.loadtxt('/content/wine.txt', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "        # here the first column is the class label, the rest are the features\n",
        "        # also convert to tensors\n",
        "        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]\n",
        "        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "\n",
        "# create dataset\n",
        "dataset = WineDataset()\n",
        "\n",
        "# get first sample and unpack\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(features, labels)\n",
        "\n",
        "# Load whole dataset with DataLoader\n",
        "# shuffle: shuffle data, good for training-> if you see the dataset, all first rows have samples for class 1, the next rows have 2, so we must suffle it!\n",
        "# num_workers: faster loading with multiple subprocesses\n",
        "# !!! IF YOU GET AN ERROR DURING LOADING, SET num_workers TO 0 !!!\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# convert the dataloader object to an iterator and look at one random sample\n",
        "dataiter = iter(train_loader)\n",
        "data = next(dataiter) \n",
        "# unpack data\n",
        "features, labels = data\n",
        "print(features, labels) # you will see 4 training samples with their corresponding labels, because we set batch size=4 when creating the dataloader\n",
        "\n",
        "# Dummy Training loop\n",
        "num_epochs = 2\n",
        "\n",
        "total_samples = len(dataset)  # it is total_samples=178, batch_size=4, so we will have 178/4=45 iterations for each epoch\n",
        "n_iterations = math.ceil(total_samples/4)\n",
        "print(total_samples, n_iterations)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "        # here you must do:\n",
        "        # 1. forward pass and compute loss\n",
        "        # 2. compute gradients and backward pass\n",
        "        # 3. update weights\n",
        "        \n",
        "        # here: 178 samples, batch_size = 4, n_iters=178/4=44.5 -> 45 iterations\n",
        "        # Run your training process\n",
        "        if (i+1) % 5 == 0:\n",
        "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3EtwgzfY2bR",
        "outputId": "c2cbc7c1-40ff-434a-927b-c6fadf682cd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]) tensor([1.])\n",
            "tensor([[1.2530e+01, 5.5100e+00, 2.6400e+00, 2.5000e+01, 9.6000e+01, 1.7900e+00,\n",
            "         6.0000e-01, 6.3000e-01, 1.1000e+00, 5.0000e+00, 8.2000e-01, 1.6900e+00,\n",
            "         5.1500e+02],\n",
            "        [1.4100e+01, 2.0200e+00, 2.4000e+00, 1.8800e+01, 1.0300e+02, 2.7500e+00,\n",
            "         2.9200e+00, 3.2000e-01, 2.3800e+00, 6.2000e+00, 1.0700e+00, 2.7500e+00,\n",
            "         1.0600e+03],\n",
            "        [1.3050e+01, 5.8000e+00, 2.1300e+00, 2.1500e+01, 8.6000e+01, 2.6200e+00,\n",
            "         2.6500e+00, 3.0000e-01, 2.0100e+00, 2.6000e+00, 7.3000e-01, 3.1000e+00,\n",
            "         3.8000e+02],\n",
            "        [1.1840e+01, 8.9000e-01, 2.5800e+00, 1.8000e+01, 9.4000e+01, 2.2000e+00,\n",
            "         2.2100e+00, 2.2000e-01, 2.3500e+00, 3.0500e+00, 7.9000e-01, 3.0800e+00,\n",
            "         5.2000e+02]]) tensor([[3.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [2.]])\n",
            "178 45\n",
            "Epoch: 1/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 1/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n",
            "Epoch: 2/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
            "Epoch: 2/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some famous datasets are available in torchvision.datasets\n",
        "# e.g. MNIST, Fashion-MNIST, CIFAR10, COCO\n",
        "# and this is how we can load them\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           train=True, \n",
        "                                           transform=torchvision.transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=3, \n",
        "                                           shuffle=True)\n",
        "\n",
        "# look at one random sample\n",
        "dataiter = iter(train_loader)\n",
        "data = next(dataiter)\n",
        "inputs, targets = data\n",
        "print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHLgUbkmY2WV",
        "outputId": "0100e4ee-9788-4d17-8b8a-1631ec5b8651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 28, 28]) torch.Size([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transforms can be applied to PIL images, tensors, ndarrays, or custom data\n",
        "during creation of the DataSet\n",
        "complete list of built-in transforms: \n",
        "http://man.hubwiz.com/docset/torchvision.docset/Contents/Resources/Documents/transforms.html\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "On Images\n",
        "---------\n",
        "CenterCrop, Grayscale, Pad, RandomAffine\n",
        "RandomCrop, RandomHorizontalFlip, RandomRotation\n",
        "Resize, Scale\n",
        "On Tensors\n",
        "----------\n",
        "LinearTransformation, Normalize, RandomErasing\n",
        "Conversion\n",
        "----------\n",
        "ToPILImage: from tensor or ndrarray\n",
        "ToTensor : from numpy.ndarray or PILImage\n",
        "Generic\n",
        "-------\n",
        "Use Lambda \n",
        "Custom\n",
        "------\n",
        "Write own class\n",
        "Compose multiple Transforms\n",
        "---------------------------\n",
        "composed = transforms.Compose([Rescale(256),\n",
        "                               RandomCrop(224)])\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "# our own custom wine dataset class\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "    def __init__(self, transform=None): # now, we include transform argument\n",
        "        xy = np.loadtxt('/content/wine.txt', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "        # note that we do not convert to tensor here, we use a class later below\n",
        "        self.x_data = xy[:, 1:]\n",
        "        self.y_data = xy[:, [0]]\n",
        "\n",
        "        # init transform\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.x_data[index], self.y_data[index]\n",
        "        \n",
        "        # if available, apply transform\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "# Custom Transforms\n",
        "# implement __call__(self, sample)\n",
        "class ToTensor:\n",
        "    # Convert ndarrays to Tensors\n",
        "    def __call__(self, sample):\n",
        "        inputs, targets = sample\n",
        "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
        "\n",
        "class MulTransform:\n",
        "    # multiply inputs with a given factor\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        inputs, targets = sample\n",
        "        inputs *= self.factor\n",
        "        return inputs, targets\n",
        "\n",
        "print('Without Transform')\n",
        "dataset = WineDataset()\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(type(features), type(labels))\n",
        "print(features, labels)\n",
        "\n",
        "print('\\nWith Tensor Transform')\n",
        "dataset = WineDataset(transform=ToTensor())\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(type(features), type(labels))\n",
        "print(features, labels)\n",
        "\n",
        "print('\\nWith Tensor and Multiplication Transform')\n",
        "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(4)])\n",
        "dataset = WineDataset(transform=composed)\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(type(features), type(labels))\n",
        "print(features, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvlfFS-NY2Sl",
        "outputId": "8c23b558-8dd4-46e2-82f1-dd4507089fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Transform\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
            " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03] [1.]\n",
            "\n",
            "With Tensor Transform\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]) tensor([1.])\n",
            "\n",
            "With Tensor and Multiplication Transform\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
            "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
            "        4.2600e+03]) tensor([1.])\n"
          ]
        }
      ]
    }
  ]
}